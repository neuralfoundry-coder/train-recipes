# Train Recipes π€

λ‹¤μ–‘ν• ν”„λ μ„μ›ν¬μ™€ κΈ°λ²•μ„ ν™μ©ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM) νμΈνλ‹ λ μ‹ν”Ό λ¨μμ…λ‹λ‹¤.

## κ°μ”

μ΄ μ €μ¥μ†λ” λ‹¤μ κΈ°λ¥μ„ κ°–μ¶ LLM νμΈνλ‹ μ¤ν¬λ¦½νΈμ™€ μ„¤μ •μ„ μ κ³µν•©λ‹λ‹¤:
- **LoRA/QLoRA**λ¥Ό ν™μ©ν• ν¨μ¨μ μΈ νμΈνλ‹
- **μ¶”λ΅  + λ€ν™”ν•** νΌν•© λ°μ΄ν„°μ…‹ ν•™μµ
- **Unsloth**λ¥Ό ν†µν• μµμ ν™”λ ν•™μµ μ†λ„
- **Wandb** μ—°λ™μΌλ΅ μ‹¤ν— μ¶”μ 

## λ””λ ‰ν† λ¦¬ κµ¬μ΅°

```
train-recipes/
β”β”€β”€ samples/
β”‚   β””β”€β”€ unsloth/
β”‚       β””β”€β”€ qwen3/                    # Qwen3-14B νμΈνλ‹
β”‚           β”β”€β”€ qwen3_(14b)_reasoning_conversational.py  # ν•™μµ λ¨λ“
β”‚           β”β”€β”€ chat_console.py       # λ€ν™”ν• μ±„ν… μ½μ†”
β”‚           β”β”€β”€ train.sh              # ν•™μµ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚           β”β”€β”€ chat.sh               # μ±„ν… μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚           β””β”€β”€ env_local             # μ„¤μ • νμΌ (git μ μ™Έ)
β”β”€β”€ reasoning/                        # μ¶”λ΅  μ¤‘μ‹¬ λ μ‹ν”Ό
β””β”€β”€ README.md
```

## λΉ λ¥Έ μ‹μ‘

### 1. μ‚¬μ „ μ”κµ¬μ‚¬ν•­

- Python 3.10+
- CUDA 11.8+ λ° νΈν™ GPU (24GB+ VRAM κ¶μ¥)
- [uv](https://docs.astral.sh/uv/) (μ—†μΌλ©΄ μλ™ μ„¤μΉ)

### 2. ν™κ²½ μ„¤μ •

```bash
cd samples/unsloth/qwen3/

# setup μ¤ν¬λ¦½νΈ μ‹¤ν–‰ (uv venv μƒμ„± λ° ν¨ν‚¤μ§€ μ„¤μΉ)
./setup.sh

# λλ” νΉμ • Python λ²„μ „ μ§€μ •
./setup.sh -p 3.11
```

### 3. μ„¤μ • κµ¬μ„±

λ μ‹ν”Ό λ””λ ‰ν† λ¦¬μ— `env_local` νμΌ μƒμ„±:

```bash
cp env_local.example env_local  # λλ” μ§μ ‘ μƒμ„±
```

`env_local` νμΌμ„ νΈμ§‘ν•μ—¬ μ„¤μ •:

```bash
# API ν† ν°
HF_TOKEN="your_huggingface_token"
WNB_API_KEY="your_wandb_api_key"

# λ¨λΈ μ„¤μ •
MODEL_NAME="unsloth/Qwen3-14B"
MAX_SEQ_LENGTH="32768"
LOAD_IN_4BIT="false"

# LoRA μ„¤μ •
LORA_R="32"
LORA_ALPHA="32"

# ν•™μµ μ„¤μ •
TRAIN_GPU_IDS="0,1"
TRAIN_BATCH_SIZE="2"
TRAIN_LEARNING_RATE="2e-4"
TRAIN_MAX_STEPS="30"

# μ¶”λ΅  μ„¤μ •
INFER_TEMPERATURE="0.7"
INFER_TOP_P="0.8"
INFER_MAX_TOKENS="16384"
```

### 4. ν•™μµ μ‹¤ν–‰

```bash
# ν„μ¬ μ„¤μ • ν™•μΈ
./train.sh -v

# env_local μ„¤μ •μΌλ΅ ν•™μµ μ‹μ‘
./train.sh

# λ…λ Ήμ¤„μ—μ„ μ„¤μ • μ¤λ²„λΌμ΄λ“
./train.sh -g 0 -b 4 -s 100 -r 1e-4
```

### 5. λ¨λΈ ν…μ¤νΈ

```bash
# μ‚¬μ© κ°€λ¥ν• ν•™μµλ λ¨λΈ λ©λ΅
./chat.sh -l

# λ€ν™”ν• μ±„ν… μ‹μ‘ (μµμ‹  λ¨λΈ μλ™ νƒμ§€)
./chat.sh

# thinking λ¨λ“λ΅ μ‹μ‘
./chat.sh -T

# νΉμ • λ¨λΈ κ²½λ΅ μ§€μ •
./chat.sh -m ./logs/qwen3-14b_reasoning-conversational_20260111_143052/lora_model
```

## ν•™μµ μ¤ν¬λ¦½νΈ

### `train.sh` μµμ…

| μµμ… | μ„¤λ… |
|------|------|
| `-g, --gpu IDS` | GPU μ¥μΉ ID (μ: `0,1`) |
| `-b, --batch N` | λ°°μΉ ν¬κΈ° |
| `-r, --lr RATE` | ν•™μµλ¥  |
| `-s, --steps N` | μµλ€ ν•™μµ μ¤ν… |
| `--lora-r N` | LoRA λ­ν¬ |
| `--no-venv` | κ°€μƒν™κ²½ ν™μ„±ν™” κ±΄λ„λ›°κΈ° |
| `-l, --logs` | μµκ·Ό ν•™μµ λ΅κ·Έ λ©λ΅ |
| `-c, --clean` | μ¤λλ λ΅κ·Έ λ””λ ‰ν† λ¦¬ μ •λ¦¬ |
| `-v, --vars` | ν„μ¬ μ„¤μ • ν‘μ‹ |

### `chat.sh` μµμ…

| μµμ… | μ„¤λ… |
|------|------|
| `-m, --model PATH` | LoRA λ¨λΈ κ²½λ΅ |
| `-g, --gpu ID` | GPU μ¥μΉ ID |
| `-t, --tokens N` | μµλ€ μƒμ„± ν† ν° μ |
| `-T, --thinking` | thinking λ¨λ“ ν™μ„±ν™” |
| `--no-venv` | κ°€μƒν™κ²½ ν™μ„±ν™” κ±΄λ„λ›°κΈ° |
| `-l, --list` | μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ |
| `-v, --vars` | μ¶”λ΅  μ„¤μ • ν‘μ‹ |

## μ±„ν… μ½μ†” λ…λ Ήμ–΄

λ€ν™”ν• μ½μ†” λ‚΄μ—μ„ μ‚¬μ© κ°€λ¥ν• λ…λ Ήμ–΄:

| λ…λ Ήμ–΄ | μ„¤λ… |
|--------|------|
| `/help` | μ‚¬μ© κ°€λ¥ν• λ…λ Ήμ–΄ ν‘μ‹ |
| `/thinking` | thinking λ¨λ“ ν† κΈ€ |
| `/clear` | λ€ν™” νμ¤ν† λ¦¬ μ΄κΈ°ν™” |
| `/mode` | ν„μ¬ μ„¤μ • ν‘μ‹ |
| `/tokens N` | μµλ€ ν† ν° μ μ„¤μ • |
| `/single` | λ‹¨μΌ ν„΄ λ¨λ“ |
| `/multi` | λ©€ν‹° ν„΄ λ¨λ“ |
| `/exit` | μ½μ†” μΆ…λ£ |

## μ¶λ ¥ κµ¬μ΅°

ν•™μµ κ²°κ³Όλ¬Όμ€ κ³ μ ν• νƒ€μ„μ¤νƒ¬ν”„μ™€ ν•¨κ» μ €μ¥λ©λ‹λ‹¤:

```
logs/
β””β”€β”€ qwen3-14b_reasoning-conversational_20260111_143052/
    β”β”€β”€ train/
    β”‚   β”β”€β”€ train.log           # ν•™μµ λ΅κ·Έ
    β”‚   β”β”€β”€ config.txt          # μ„¤μ • μ¤λƒ…μƒ·
    β”‚   β”β”€β”€ training_stats.txt  # μµμΆ… ν†µκ³„
    β”‚   β””β”€β”€ tensorboard/        # TensorBoard λ΅κ·Έ
    β”β”€β”€ eval/
    β”‚   β””β”€β”€ dataset_info.txt    # λ°μ΄ν„°μ…‹ ν†µκ³„
    β”β”€β”€ checkpoints/            # ν•™μµ μ²΄ν¬ν¬μΈνΈ
    β””β”€β”€ lora_model/             # μµμΆ… LoRA μ–΄λ‘ν„°
```

## μ‚¬μ© κ°€λ¥ν• λ μ‹ν”Ό

### Qwen3-14B μ¶”λ΅  + λ€ν™”ν•

Qwen3-14Bλ¥Ό λ‹¤μ λ°μ΄ν„° νΌν•©μΌλ΅ νμΈνλ‹:
- **μ¶”λ΅  λ°μ΄ν„° (75%)**: [OpenMathReasoning-mini](https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini)
- **λ€ν™”ν• λ°μ΄ν„° (25%)**: [FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k)

νΉμ§•:
- thinking λ° non-thinking μ¶”λ΅  λ¨λ“ λ¨λ‘ μ§€μ›
- κ° λ¨λ“μ— μµμ ν™”λ μƒμ„± νλΌλ―Έν„°
- μ™„μ „ν• λ΅κΉ… λ° μ‹¤ν— μ¶”μ 

## μ„¤μ • μ°μ„ μμ„

μ„¤μ •μ€ λ‹¤μ μμ„λ΅ λ΅λ“λ©λ‹λ‹¤ (λ†’μ€ μ°μ„ μμ„ λ¨Όμ €):

1. **λ…λ Ήμ¤„ μΈμ**
2. **ν™κ²½ λ³€μ** (`export TRAIN_BATCH_SIZE=4`)
3. **env_local νμΌ**
4. **μ½”λ“ λ‚΄ κΈ°λ³Έκ°’**

## λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” LGPL-3.0 λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤.

## κ°μ‚¬μ κΈ€

- [Unsloth](https://github.com/unslothai/unsloth) - μµμ ν™”λ ν•™μµ μ κ³µ
- [Hugging Face](https://huggingface.co/) - λ¨λΈ νΈμ¤ν…
- [Weights & Biases](https://wandb.ai/) - μ‹¤ν— μ¶”μ 

